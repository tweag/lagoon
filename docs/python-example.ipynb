{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcing: Lagoon\n",
    "\n",
    "We are happy to announce the open source release of [Lagoon](https://github.com/tweag/lagoon). Codeveloped with Pfizer, Lagoon is a tool for centralizing semi-structured datasets like CSV or JSON files into a data “lagoon”, where your data can be easily versioned, queried, or passed along to other ETL pipelines.\n",
    "\n",
    "### Overview\n",
    "\n",
    "Lagoon was created to make large sets of delimited text and JSON files with hetereogeneous schemas easier to query and integrate into data analysis and data science workflows. The primary component of Lagoon is its server which is layered on top of a Postgres database. Making use of Haskell’s type system, the Lagoon server automatically generates schemas for your datasets, allowing you to directly ingest them without having to manually configure database tables. Data is queryable via a REST API, client libraries, or directly in Postgres via automatically generated SQL views.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Support for ingestion of JSON and delimited text files\n",
    "- Data versioning - store and query multiple versions of the same dataset\n",
    "- SQL queries via a REST API or directly in Postgresql via automatically generated SQL views\n",
    "- Command line, Python, Ruby, and R clients\n",
    "- Easy integration with other applications and ETL tools via the REST and SQL interfaces\n",
    "- Can serve as a hub for data analysis projects or a starting point for other ETL pipelines\n",
    "\n",
    "### Why use Lagoon?\n",
    "\n",
    "While other tools like [Apache Drill](https://drill.apache.org/) also provide cross-dataset querying capabilities, they typically require the user to manually specify types for data stored in text-based formats. Lagoon’s type inference makes the process of querying these datasets smoother, and since data is ingested into a centralized relational database, it’s easier to integrate that data with traditional ETL tools.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's try it out! As a simple example, we can ingest and query a few datasets from the [NOAA Storm Events Database](https://www.ncdc.noaa.gov/stormevents) which contains a record of major storm events in the United States over the past century. In this example, we will:\n",
    "  1. Start up a lagoon server and database backend using [docker](https://docs.docker.com/get-docker/) and [docker-compose](https://docs.docker.com/compose/)\n",
    "  2. Ingest a few example files into our new lagoon using the [lagoon-client](https://hub.docker.com/r/tweag/lagoon-client) docker image\n",
    "  3. Query and plot some data from our newly ingested storm datasets using Lagoon's [python client library](https://github.com/tweag/lagoon/tree/master/clients/PyLagoon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a new lagoon\n",
    "We can create a new lagoon instance using the docker-compose file that is included in the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Note: we are assuming that this example is being run from the lagoon/docs directory\n",
    "cd ../docker\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ingest our example datasets\n",
    "\n",
    "Now that the lagoon server is running, we can ingest some example datasets. One easy way to ingest data is via the [lagoon-client]() docker image. \n",
    "\n",
    "Let's take a look at the storms from 2019.  The data files in this example can be downloaded\n",
    "from the NOAA storms file server at https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Note: these commands assume you've downloaded the csv files to your working directory\n",
    "docker run --rm --network=\"host\" --volume \"$PWD/StormEvents_details-ftp_v1.0_d2019_c20200716.csv:/StormEvents_details-ftp_v1.0_d2019_c20200716.csv\" \\\n",
    "    tweag/lagoon-client --port 1234 --host localhost ingest --comma -n storm_details_2019 /StormEvents_details-ftp_v1.0_d2019_c20200716.csv\n",
    "\n",
    "docker run --rm --network=\"host\" --volume \"$PWD/StormEvents_fatalities-ftp_v1.0_d2019_c20200716.csv:/StormEvents_fatalities-ftp_v1.0_d2019_c20200716.csv\" \\\n",
    "    tweag/lagoon-client --port 1234 --host localhost ingest --comma -n storm_fatalities_2019 /StormEvents_fatalities-ftp_v1.0_d2019_c20200716.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Querying data\n",
    "\n",
    "With our data ingested we can start querying data using PyLagoon and analyze it using some standard data science tools in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyLagoon import LagoonConfig, Lagoon, PGMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is initialize our client using the example config provided with the lagoon repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lagoon = Lagoon(\n",
    "    config=LagoonConfig.load(yaml_file=\"../docker/examples/lagoon-client.yaml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can access our datasets using the names we provided when we ingested them. You can also query for all data sources by ommitting the `name` kwarg or query by tag using the `tag` kwarg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_source = lagoon.sources(name=\"storm_details_2019\")[0]\n",
    "fatalities_source = lagoon.sources(name=\"storm_fatalities_2019\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each source contains a description of its corresponding dataset, but no actual data has been loaded yet. To load data into a pandas DataFrame, we can use our `lagoon` object's `download_query()` or `download_source()` methods.\n",
    "\n",
    "Let's query all of the storms that happened in Texas in 2019.  One the advantages to using Lagoon is that we can limit the data we load into memory to records we're interested in by specifying a SQL query.  This allows us to limit client resource consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagoon uses SQLAlchemy for formatting SQL queries: https://www.sqlalchemy.org/\n",
    "\n",
    "# To construct queries using SQLAlchemy, we need to generate a description of our database schema\n",
    "meta = PGMeta([details_source, fatalities_source])\n",
    "\n",
    "# Schemas for our two datasets:\n",
    "storms = meta[details_source]\n",
    "fatalities = meta[fatalities_source]\n",
    "\n",
    "# Note: we can use the PyLagoon.build_sql_query() function to preview or spot-check our query\n",
    "query = meta.query(storms).filter(storms.STATE.like(\"%TEXAS%\"))\n",
    "\n",
    "df = lagoon.download_query(query=query, sources=[details_source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our query results loaded, we can start working with our dataset. For example, we can map the storms along with their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/\n",
    "# https://plotly.com/python/\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# It looks like some bad values in the lat/lon columns forced them \n",
    "# to be stored as strings. We can still cast them to floats (ignoring \n",
    "# errors) using pandas:\n",
    "df[\"BEGIN_LAT\"] = pd.to_numeric(df[\"BEGIN_LAT\"])\n",
    "df[\"BEGIN_LON\"] = pd.to_numeric(df[\"BEGIN_LON\"])\n",
    "\n",
    "px.scatter_mapbox(\n",
    "    df, \n",
    "    lat=\"BEGIN_LAT\", \n",
    "    lon=\"BEGIN_LON\",\n",
    "    color=\"EVENT_TYPE\",\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    color_continuous_scale=px.colors.cyclical.IceFire,\n",
    "    size_max=15,\n",
    "    zoom=5,\n",
    "    title=\"NOAA Storm Events (2019)\",\n",
    "    width=1000,\n",
    "    height=800\n",
    ").show()\n",
    "\n",
    "px.histogram(df, x=\"EVENT_TYPE\", width=1000, height=600).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Texas has a lot of hail storms!\n",
    "\n",
    "We can also perform more complex queries. For example, we can join our two datasets to see the type of location where the most storm-related fatalities occurred in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    meta.query(\n",
    "        storms.EVENT_ID, storms.BEGIN_LAT, storms.BEGIN_LON, fatalities.FATALITY_LOCATION\n",
    "    )\n",
    "    .join(fatalities, storms.EVENT_ID == fatalities.EVENT_ID)\n",
    "    .filter(storms.STATE.like(\"%TEXAS%\"))\n",
    ")\n",
    "df_joined = lagoon.download_query(query=query, sources=[details_source, fatalities_source])\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_joined, \n",
    "    x=\"FATALITY_LOCATION\",\n",
    "    title=\"Locations of storm-related fatalities (2019)\",\n",
    "    width=1000, \n",
    "    height=600\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few quick commands we were able to ingest new datasets into our lagoon and start querying them, all without having to worry about generating database schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "The Lagoon server and command line client are available as Docker images [on DockerHub](https://hub.docker.com/u/tweag), and all components are also packaged using Nix in the [Lagoon GitHub repository](https://github.com/tweag/lagoon).\n",
    "\n",
    "To get started with lagoon, check out the lagoon [documentation](https://github.com/tweag/lagoon) on GitHub. Thanks for reading, and we hope that Lagoon is able to help streamline your data analysis workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python361064bitvenvvenvb784b78c351f4179b5984a56e4df19e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}